\section{Data understanding} \label{seq:data_understanding}

 In this step the objective is to consider the available data, understand their properties, check its quality and explore it through statistical methods.

 \subsection{Collect initial data}

The data is collected through a platform called \textit{ballchasing.com} that allows players to download a plugin for the game that will automatically load their replays to to platform.

The replays are then viewable into a web based frontend and is possible to analyze different statistics from the replay. The platform provides also different HTTP API:

\begin{itemize}
    \item Get replay: returns the binary replay file given the ID;
    \item Get replay list: returns a json involving summary of replays (including the ID to get the full replay) given different filters;
    \item Get replay statistics: returns a full detailed statistics of the game for each player given the replay ID;
    \item Upload replay: used by the plugin to upload replays;
    \item Delete replay.
\end{itemize}

It is important to clarify that a replay is not a video file as usual, but a large binary file containing various metadata on the match and the players and a table in which are listed all the information necessary to review the match.
Therefore, for each instant \textit{(frame)} of the game, are listed position, direction and velocity vector of each player and the ball, and also other information of the input the players (e.g. player is using boost, player is drifting, etc...).

Extracting features from the raw replays can be very challenging, and the amount of possible features is quite high. However, the main problem about raw replays is API limitations, as it is possible to download only 200 replays per hour. Instead, we can download the replay statistics, as they are very detailed and provide stats about all the players, furthermore the limitation is 2000 per hour.

Thus, in this project, we used the calculated stats from the APIs. Each downloaded replay is saved in a JSON file. These, are hierarchical files contain also a lot of metadata and other useless for our task information about the match (e.g. stadium, car personalization of each player etc...). We discarded this data and take the stats about the six players in the match, which then results in 6 rows for each replay.

Data was collected using \textit{random sampling} from the date. The date is sampled from a range starting from August 2021 until February 2022, that's because are the dates of the two last \textit{"Seasons"} of Rocket League. At the end of each season there is a soft reset of ranks: the MMR doesn't change but the players have to do again 10 matches where the MMR change at the end of the match is higher.
I decided to not consider older seasons because the \textit{playstyle} and the distribution of players in Rocket League changes over time.

However, we have to discard some player rows where the rank information was missing. This is due to the fact that the rank in that match wasn't determined yet.

\subsection{Describe data and features}

The resulting dataset counts \note{data len} rows and 85 features in our data, let's briefly describe them:

\input{res/feature_list.tex}

We can right away notice that there are a lot of features correlated, all the time / percent features are a repetition and we could keep only one among the twos.

Among these we can distinguish \textbf{categorical} and \textbf{numeric features} (\reftab{tab:catdiscr}):

\input{res/tables/catdiscr.tex}

\subsection{Verify data quality}

All of our data is automatically collected, thus, is not subject to human errors.
We cannot ensure that data is \textit{accurate}, so that stored value is the actual value, but we can assume it always because of the collection process.

Data is \textit{incomplete}. There where a bunch of missing data in some dozen of rows in columns:\textit{percent\_closest\_to\_ball} \textit{percent\_farthest\_from\_ball}, \textit{percent\_most\_forward}, \textit{avg\_distance\_to\_mates}, \textit{time\_most\_back}.

Missing values in this case are originated from leaving the match before the end, causing errors in the calculation of the metrics. We can notice that most of the missing values are in Bronze ranks, because they tend to care less about their rank and the game in general (leaving a ranked match will count as a loss and will ban you for short period from the ranked matches).

Data is \textit{consistent}: the only measurement is time, and it is always in seconds. All other features are just numbers.
Data is \textit{up-to date}: is collected from replays of the two last seasons, and, until the game mechanisms change, it isn't obsolete.

\subsection{Clean data}

In order to calculate summary statistics on our data we need to get rid of missing values and errors in our data.
In this case the most appropriate solution is also the most simple, so, removing this rows from the dataset.

\subsection{Data exploration}

Let's see some statistics from our data:

\begin{figure}[H]
    \resizebox{1.1\linewidth}{!}{\includegraphics{res/imgs/tiers.pdf}}
    \label{fig:rank_distr}
    \caption{Distribution of players ranks (blue) vs. distribution of ranks in data (orange)}
\end{figure}

We can notice that the the bronze class had very few examples. This has two main reasons, the first one is that there are actually very few players in bronze, it's very difficult to get there basing on how ranking system works. The second reason is that players in bronze are casuals players that even don't know about the plugin necessary to upload replays. We can see the distribution of ranks for the current season (Season 5) in \reffig{fig:rank_distr} (orange). In the same figure we can see in blue the actual distribution of players, that is more or less normally distributed and centered. The data distribution is instead shifted to the right; as we said, skilled players tends to use and know about the plugin.

\section{Data preparation}

Data selection was automatically performed during the collection of the dataset. As already said, replays are randomly sampled between two dates representing two Rocket League seasons.

For feature selection the situation is different, the number of features is quite high and we can early see that some of the are correlated, thus we plot a correlation matrix \reftab{fig:corr_matrix}:

\begin{figure}[H]
    \resizebox{1.1\linewidth}{!}{\includegraphics{res/imgs/correlations.pdf}}
    \label{fig:corr_matrix}
    \caption{Features correlation matrix}
\end{figure}

\input{res/tables/corr.tex}

By this selection we removed 42 features, thus, nearly halving the dataset vertically. The adopted criterion is based on logical correlation, thus, by inspecting correlations, the removed features are the ones where we can find a motivation for the correlation, otherwise the correlation can be spurious.

In order to further reduce the dimensionality of the dataset another two tests have been ran: the information gain test and the chi square test, results are shown in \reffig{fig:ig} and \reffig{fig:chi_square}:

\begin{figure}[H]
    \resizebox{1.1\linewidth}{!}{\includegraphics{res/imgs/ig.pdf}}
    \label{fig:ig}
    \caption{Information gain results}
\end{figure}

\begin{figure}[H]
    \resizebox{1.1\linewidth}{!}{\includegraphics{res/imgs/chi2.pdf}}
    \label{fig:chi_square}
    \caption{Chi square test results} 
\end{figure}

The results gave another insight on some kind of features uninformative. The features that are strictly related to the specific match and not on the playstyle and skill of the player. Such features are \textit{goals}, \textit{mvp}, \textit{assists} and \textit{goals\_against\_while\_last\_defender}.
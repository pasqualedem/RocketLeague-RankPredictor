\section{Data understanding} \label{seq:data_understanding}

 In this step the objective is to consider the available data, understand their properties, check its quality and explore it through statistical methods.

 \subsection{Collect initial data}

The data is collected through a platform called \textit{ballchasing.com} that allows players to download a plugin for the game that will automatically load their replays to to platform.

The replays are then viewable into a web based frontend and is possible to analyze different statistics from the replay. The platform provides also different HTTP API:

\begin{itemize}
    \item Get replay: returns the binary replay file given the ID;
    \item Get replay list: returns a json involving summary of replays (including the ID to get the full replay) given different filters;
    \item Get replay statistics: returns a full detailed statistics of the game for each player given the replay ID;
    \item Upload replay: used by the plugin to upload replays;
    \item Delete replay.
\end{itemize}

It is important to clarify that a replay is not a video file as usual, but a large binary file containing various metadata on the match and the players and a table in which are listed all the information necessary to review the match.
Therefore, for each instant \textit{(frame)} of the game, are listed position, direction and velocity vector of each player and the ball, and also other information of the input the players (e.g. player is using boost, player is drifting, etc...).

Extracting features from the raw replays can be very challenging, and the amount of possible features is quite high. However, the main problem about raw replays is API limitations, as it is possible to download only 200 replays per hour. Instead, we can download the replay statistics, as they are very detailed and provide stats about all the players, furthermore the limitation is 2000 per hour.

Thus, in this project, we used the calculated stats from the APIs. Each downloaded replay is saved in a JSON file. These, are hierarchical files contain also a lot of metadata and other useless for our task information about the match (e.g. stadium, car personalization of each player etc...). We discarded this data and take the stats about the six players in the match, which then results in 6 rows for each replay.

Data was collected using \textit{random sampling} from the date. The date is sampled from a range starting from August 2021 until February 2022, that's because are the dates of the two last \textit{"Seasons"} of Rocket League. At the end of each season there is a soft reset of ranks: the MMR doesn't change but the players have to do again 10 matches where the MMR change at the end of the match is higher.
I decided to not consider older seasons because the \textit{playstyle} and the distribution of players in Rocket League changes over time.

However, we have to discard some player rows where the rank information was missing. This is due to the fact that the rank in that match wasn't determined yet.

\subsection{Describe data and features}

The resulting dataset counts \note{data len} rows and 85 features in our data, let's briefly describe them:

\input{res/feature_list.tex}

We can right away notice that there are a lot of features correlated, all the time / percent features are a repetition and we could keep only one among the twos.

Among these we can distinguish \textbf{categorical} and \textbf{numeric features} (\reftab{tab:catdiscr}):

\input{res/tables/catdiscr.tex}

\subsection{Verify data quality}

All of our data is automatically collected, thus, is not subject to human errors.
We cannot ensure that data is \textit{accurate}, so that stored value is the actual value, but we can assume it always because of the collection process.

Data is \textit{incomplete}. There where a bunch of missing data in some dozen of rows in columns:\textit{percent\_closest\_to\_ball} \textit{percent\_farthest\_from\_ball}, \textit{percent\_most\_forward}, \textit{avg\_distance\_to\_mates}, \textit{time\_most\_back}.

Missing values in this case are originated from leaving the match before the end, causing errors in the calculation of the metrics. We can notice that most of the missing values are in Bronze ranks, because they tend to care less about their rank and the game in general (leaving a ranked match will count as a loss and will ban you for short period from the ranked matches).

Data is \textit{consistent}: the only measurement is time, and it is always in seconds. All other features are just numbers.
Data is \textit{up-to date}: is collected from replays of the two last seasons, and, until the game mechanisms change, it isn't obsolete.

\subsection{Clean data}

In order to calculate summary statistics on our data we need to get rid of missing values and errors in our data.
In this case the most appropriate solution is also the most simple, so, removing this rows from the dataset.

\subsection{Data exploration}

Let's see some statistics from our data:

\begin{figure}[H]
    \resizebox{1.1\linewidth}{!}{\includegraphics{res/imgs/tiers.pdf}}
    \label{fig:rank_distr}
    \caption{Distribution of players ranks (blue) vs. distribution of ranks in data (orange)}
\end{figure}

We can notice that the the bronze class had very few examples. This has two main reasons, the first one is that there are actually very few players in bronze, it's very difficult to get there basing on how ranking system works. The second reason is that players in bronze are casuals players that even don't know about the plugin necessary to upload replays. We can see the distribution of ranks for the current season (Season 5) in \reffig{fig:rank_distr} (orange). In the same figure we can see in blue the actual distribution of players, that is more or less normally distributed and centered. The data distribution is instead shifted to the right; as we said, skilled players tends to use and know about the plugin.

\section{Data preparation}

Data selection was automatically performed during the collection of the dataset. As already said, replays are randomly sampled between two dates representing two Rocket League seasons.

For feature selection the situation is different, the number of features is quite high and we can early see that some of the are correlated, thus we plot a correlation matrix \reftab{fig:corr_matrix}:

\begin{figure}[H]
    \resizebox{1.1\linewidth}{!}{\includegraphics{res/imgs/correlations.pdf}}
    \label{fig:corr_matrix}
    \caption{Features correlation matrix}
\end{figure}

\input{res/tables/corr.tex}

By this selection we removed 42 features, thus, nearly halving the dataset vertically. The adopted criterion is based on logical correlation, thus, by inspecting correlations, the removed features are the ones where we can find a motivation for the correlation, otherwise the correlation can be spurious.

In order to further reduce the dimensionality of the dataset another two tests have been ran: the information gain test and the chi square test, results are shown in \reffig{fig:ig} and \reffig{fig:chi_square}:

\begin{figure}[H]
    \resizebox{1.1\linewidth}{!}{\includegraphics{res/imgs/ig.pdf}}
    \label{fig:ig}
    \caption{Information gain results}
\end{figure}

\begin{figure}[H]
    \resizebox{1.1\linewidth}{!}{\includegraphics{res/imgs/chi2.pdf}}
    \label{fig:chi_square}
    \caption{Chi square test results} 
\end{figure}

The results gave another insight on some kind of features uninformative. The features that are strictly related to the specific match and not on the playstyle and skill of the player. Such features are \textit{goals}, \textit{mvp}, \textit{assists} and \textit{goals\_against\_while\_last\_defender}.

\subsection{Data construction}

In this step there will be gonna built the final dataset provided to the models.

Features of this dataset have different measurement units and scales, therefore features with bigger scales will be considered more important for some models, for example linear regression. We need to scale the data.
The used scaling technique is called \textit{Robust Scaling}. It subtract the median and divides using the \textit{interquantile range (IQR)} instead of the mean and the standard deviation differently from normalization. It has been used the IQR between the 1st quartile (25th quantile) and the 3rd quartile (75th quantile).
This procedure has been applied to all the features except for some groups that needed particular attention.
The first groups are the \textbf{percentual} features, they range from 0 to 100, therefore the normalization is useless; they have been simply divided by 100 to make them range from 0 to 1. 

Other groups are:
\begin{itemize}
    \item \textit{avg\_distances}: which includes distance to ball with and without possession and distance to mates
    \item \textit{overfill}: which includes amount of boost overfill, amount overfill stolen and stolen with big pads
\end{itemize}

These groups of features have relations and constraints that can be removed by the normalization, for example amount\_overfill is always greater than amount\_overfill\_stolen, and their difference is important. Therefore, each group, has been standardized like it is a unique features, in order to keep the relations


\subsubsection{Factor analysis}

Factor analysis is a linear statistical model. It is used to explain the variance among the observed variable and condense a set of the observed variable into the unobserved variable called factors. Observed variables are modeled as a linear combination of factors and error terms (Source). Factor or latent variable is associated with multiple observed variables, who have common patterns of responses. Each factor explains a particular amount of variance in the observed variables. It helps in data interpretations by reducing the number of variables.
Factor Analysis addresses the problem of analyzing the structure of the interrelationships, among a large number of variables by defining a set of common underlying dimensions, known as \textbf{factors}. In this case, the analysis is suitable, and could improve the final metrics of the classifiers.

Firstly, it is ran a statistical test, the \textbf{Bartlett test of Sphericity}; the result of the \textit{p-value} is 0.0, which is below the significance level, that is 0.01, this means that our data is suitable for PCA or factor analysis, in other terms, the correlation matrix is not an identity matrix, therefore there are variables that can be compressed because are correlated.
After that, it another statistical test, the \textbf{Kaiser-Meyer-Olin} test. KMO estimates the proportion of variance among all the observed variables. Lower proportion is more suitable for factor analysis. KMO values range between 0 and 1. Value of KMO less than 0.6 is considered inadequate. The resulting value is 0.74, which is not excellent but still good to perform factor analysis.

The next step is choosing the number of factors, it is possible to use the Kaiser criterion or the scree plot.

\begin{lstlisting}[caption=Eigenvalues vector, label=lst:eig, numbers=none]
    8.23560479, 4.23322434, 3.22683551, 2.81685919, 2.30287244,
    1.89468945, 1.77176288, 1.32000825, 1.24627339, 1.00624468,
    0.96403381, 0.92449632, 0.84269987, 0.82747457, 0.77226586,
    0.76199741, 0.69723765, 0.69105217, 0.67808479, 0.60368556,
    0.59190556, 0.57496769, 0.51825302, 0.46937353, 0.37548574,
    0.34613834, 0.32115293, 0.3107348 , 0.27012021, 0.2462839 ,
    0.22761797, 0.18568548, 0.1833516 , 0.14390633, 0.12192532,
    0.11567416, 0.08087169, 0.03868107, 0.02639674, 0.02272699,
    0.01134398
\end{lstlisting}

We can see \reflst{lst:eig}, that there are only 8 values above 1.

\begin{figure}[H]
    \resizebox{1.1\linewidth}{!}{\includegraphics{res/imgs/scree.pdf}}
    \label{fig:scree}
    \caption{Scree plot} 
\end{figure}

Also from the scree plot \reffig{fig:scree}, the elbow is around 8, so we can state the 8 factors should be adequate.
So we perform the factor analysis and print the \textit{factor loadings} \reffig{fig:loadings}.

\begin{figure}[H]
    \resizebox{1.1\linewidth}{!}{\includegraphics{res/imgs/loadings.png}}
    \label{fig:loadings}
    \caption{Loadings for each factor that are greater than 0.5 in absolute value} 
\end{figure}

For sure we can say that variable in factor 6 and 7 are related and variables in factor 3 are all percentage of boost. For factor 5 and 2, we have two groups of related variables. Variables in factor 4 are all amount of boosts. Lastly, for factors 0 and 1 there are some related variables, but others doesn't have a clear interpretation of their relations.

\subsection{Final produced datasets}

From the preparation phase we produced 5 datasets:

\begin{itemize}
    \item Dataset selected: output of feature selection, without preprocessing
    \item Dataset preprocessed: the dataset produced by scaling;
    \item Dataset grouped preprocessed: the dataset produced by scaling done with grouping some variables;
    \item Dataset merged preprocessed: the dataset produced by scaling and merging the tier classes;
    \item Dataset merged factored: the dataset produced by merging the tier classes and performing a factor analysis
\end{itemize}
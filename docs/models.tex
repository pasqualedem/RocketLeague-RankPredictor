\section{Modeling}

In this phase, data mining models are chosen and an evaluation plan is designed. After that, models are built, trained and evaluated.
\subsection{Select modeling technique}

\subsubsection{Model representation}

According to \refseq{sec:min_goal}, our task is regression, thus \textit{supervised learning}. Also, it is necessary to consider that there are numerical features. From the plethora of models, has been chosen firstly Logistic Regression, according to Occam razor, always start from simpler models. Then have been select others that from literature, are known as accurate models, such as, Naive Bayes, k-Nearest Neighbors, Random forest, XGBoost and Multi Layer Perceptron. 
In particular, for Naive Bayes, it was used the Gaussian Naive Bayes, suitable for numeric features.


\subsection{Generate Test Design}

After selecting the models, it is crucial to chose appropriate technique for assessing the performances of the models.

\subsubsection{Metrics}

First of all, it is needed to chose the metrics to calculate the performances. In this case, having a regression, the most used metrics are \textit{Mean Absolute Error (MAE)} and \textit{Root Mean Squared error (RMSE)}. The guidance metric to choose models will be RMSE, in order to penalize more greater errors.

\subsubsection{Evaluation technique}

The dataset is firstly split into training and test, 2/3 for the training and 1/3 for the test, so the method used is the \textit{holdout}. 
As validation technique, K-fold cross validation is chosen for parameter selection, with K=10, as is a popular number of folds in literature.
Thus, for each model, after the best parameters have been found, it is retrained on the whole training set and then tested.
Having 404951 rows, means that we be trained on 269968 examples and test on 269967. In the validation, each fold will be of 13498 samples.

\subsection{Build model}

In this section there will be reported the results for each model, and each preprocessed dataset.
Grid search has been performed over all the hyperparameters chosen.


\subsubsection{Dummy Regressor}

A random regressor has been tested to check the gain of the other regressors. Two strategy have been applied: \textbf{Median} and \textbf{Mean}. Results in \reftab{tab:val_dummy}.
Note that results on different preprocessing are obviously the same.

\input{res/tables/val_tables/dummy.tex}

\subsubsection{KNN Classifier}

K-Nearest Neighbors classifier, with two different K. Results in \reftab{tab:val_knn}

\input{res/tables/val_tables/knn.tex}

\subsubsection{Linear regressor}

Attempt to fit a simple model for this task. Results in \reftab{tab:val_linear}

\input{res/tables/val_tables/linear.tex}


\subsubsection{Multi Layer Perceptron}

MLP classifier, with batch size of 256 and 3 hidden layers, the first with 256 units, the second with 128 and the last with 64. Results in \reftab{tab:val_mlp}

\input{res/tables/val_tables/mlp.tex}

\subsubsection{Gaussian Naive Bayes}

Gaussian Naive Bayes regressor, with a variable smoothing of 1e-09. Class priors given are the relative frequencies for each "classes". The term can be mislead as we are in practice making regression, but in this case we are building the predictor as a classifier.
Results in \reftab{tab:val_nb}

\input{res/tables/val_tables/nb.tex}


\subsubsection{Random forest}

Random forest regressor.
Results in \reftab{tab:val_rf}

\input{res/tables/val_tables/random_forest.tex}


\subsubsection{XGBoost}

XGBoost regressor.
Results in \reftab{tab:val_xgb}

\input{res/tables/val_tables/xgboost.tex}

\subsection{Model comparison}

After the hyperparameters have been selected from the validation and the final models have been trained on the whole training set, their results have to be compared. We can view them in \reftab{tab:test_res}.
MLP regressor outperformed every other regressor with every preprocessing type. Despite its simplicity, the linear model is just below the MLP and had better results than most complex models such as XGBoost and RandomForests in a very short fit time. KNN, in addition to have poor results, has a long score time, therefore it isn't the most suitable for real time applications.

\input{res/tables/test.tex}

